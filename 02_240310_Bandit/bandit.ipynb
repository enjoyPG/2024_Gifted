{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enjoyPG/2024_Gifted/blob/main/02_240310_Bandit/bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUJwZHWqGlQZ"
      },
      "source": [
        "# 강화 학습 환경 복습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RYvyfEy3GlQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b33040-0008-4118-d3ae-67f1f2831b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic-control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/953.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBm9msrrGlQe"
      },
      "source": [
        "### 강화 학습 문제를 직접 풀어낼 정책 정의\n",
        "\n",
        "강화 학습에서는 어떤 함수를 학습하고자 하는 걸까요? 에이전트 안에는 상태 관측값(입력)을 받고 그것을 앞으로 취해야 할 최적의 행동(출력)에 매핑하는 함수가 있습니다. 예를 들어, 미로 속 에이전트의 현재 상태가 $(2, 3)$ 좌표라면, 에이전트 안의 함수는 이 입력값을 \"오른쪽으로 이동\"이라는 출력값에 매핑하는 것이 될 수 있습니다. 이 함수를 $\\pi$라고 한다면, 아래와 같이 수식으로 쓸 수 있습니다.\n",
        "$$\n",
        "\\pi((2, 3)) = \\text{\"오른쪽으로 이동\"}\n",
        "$$\n",
        "강화 학습 용어로 이 함수를 정책(policy)이라고 부릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odg1de0dGlQf"
      },
      "outputs": [],
      "source": [
        "def policy(state):\n",
        "    x_position = state[0]\n",
        "    x_velocity = state[1]\n",
        "\n",
        "    if x_velocity > 0:\n",
        "        action = 2\n",
        "    elif x_velocity < 0:\n",
        "        action = 0\n",
        "    else:\n",
        "        if x_position > -0.6:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 2\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVhsenHaGlQf"
      },
      "source": [
        "### 강화 학습이 돌아가는 환경의 코드 복습\n",
        "\n",
        "1. 인공지능 모델은 환경의 현재 상태(state)를 관찰할 수 있습니다. 미로 찾기 문제에서 환경의 현재 상태란 미로 속 현재 위치를 의미합니다. 예를 들어, 모델이 미로의 $(2, 3)$ 좌표에 있다면, 이 좌표는 현재 상태를 나타냅니다.\n",
        "\n",
        "2. 인공지능 모델은 관찰된 상태로부터 앞으로 취할 행동(action)을 결정합니다. 양갈래 길 중에서 어디로 갈지 결정하는 것 등이 그 예시가 될 수 있습니다.\n",
        "\n",
        "3. 환경은 상태를 변경(transition)시키고 그 행동에 대한 보상(reward)을 생성합니다. 인공지능 모델은 그 상태와 보상을 다 받습니다. 미로 찾기 문제에서 환경의 변화란 인공지능 모델의 (앞선 결정에 따른) 미로 속 위치 변화를 의미합니다. 예를 들어, '오른쪽으로 이동' 행동을 취하면, 에이전트의 위치 좌표가 $(2, 3)$에서 $(2, 4)$로 바뀔 수 있습니다. 보상은 출구를 찾았을 때 주어지는 경품이나 막다른 길에 도달했을 때 받는 페널티 등을 생각해 볼 수 있습니다.\n",
        "\n",
        "4.  이 새로운 정보(환경의 변화와 이에 따른 보상)를 사용하여 인공지능은 그런 행동이 좋아 그걸 반복해야 하는지, 또는 좋지 않아 회피해야 하는지 결정할 수 있습니다. 완료될 때까지 (done) 이 관측-행동-보상 사이클은 계속됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIifUZzTGlQg"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "state, _ = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action = policy(state)\n",
        "    print(\"Chose action:\", action)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(\"New state:\", state)\n",
        "    print(\"Reward:\", reward)\n",
        "\n",
        "print(\"Final state:\", state)\n",
        "print(\"Total reward:\", total_reward)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpCeUURYGlQg"
      },
      "source": [
        "### 환경 직접 만들어보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_KbEPG3GlQh"
      },
      "outputs": [],
      "source": [
        "class MyEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.observation_space = gym.spaces.Discrete(7, start=-3)\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def reset(self):\n",
        "        state = 0\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.num_steps += 1\n",
        "\n",
        "        if action == 0:\n",
        "            next_state = state - 1\n",
        "        else:\n",
        "            next_state = state + 1\n",
        "\n",
        "        if next_state > 3:\n",
        "            next_state = 3\n",
        "        elif next_state < -3:\n",
        "            next_state = -3\n",
        "\n",
        "        reward = {\n",
        "            -3: 1,\n",
        "            -2: 1,\n",
        "            -1: 1,\n",
        "            0: 0,\n",
        "            1: -1,\n",
        "            2: -1,\n",
        "            3: 10\n",
        "        }[next_state]\n",
        "\n",
        "        done = self.num_steps >= 3\n",
        "        return next_state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08bHyZglGlQh"
      },
      "outputs": [],
      "source": [
        "class StudentMDP(gym.Env):\n",
        "    def __init__(self):\n",
        "        # 0: 수업, 1: 야자, 2: 집, 3: 잘침, 4: 못침\n",
        "        self.observation_space = gym.spaces.Discrete(5)\n",
        "\n",
        "        # 0: 공부, 1: 딴짓, 2: 땡땡이, 3: 쇼츠보기, 4: 벼락치기, 5: 수면\n",
        "        self.action_space = gym.spaces.Discrete(6)\n",
        "\n",
        "    def reset(self):\n",
        "        state = ???\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        state = ???\n",
        "        reward = ???\n",
        "        done = ???\n",
        "        return state, reward, done, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKdf5eVmGlQi"
      },
      "source": [
        "# 슬롯머신 정복하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mulU5ZrmGlQi"
      },
      "source": [
        "### 슬롯머신 환경 구현하기\n",
        "\n",
        "각각의 밴딧 $i = 1, \\cdots, n$에 대해, 먼저 랜덤하게 실제 가치 $\\mu_{i} \\sim N(0, 1)$를 평균 0 분산 1인 표준정규분포에서 추출해 정해줍니다. 그리고 $i$번 째 밴딧을 고를 경우, 받을 수 있는 보상은 다음과 같이 정해줍니다:\n",
        "$$\n",
        "r_{t} = \\mu_{i} + \\varepsilon \\quad \\text{where} \\quad \\varepsilon \\sim N(0, 1)\n",
        "$$\n",
        "즉, $i$번 째 밴딧을 골랐을 때의 보상은, 그 밴딧의 실제 가치 $\\mu_{i}$에 랜덤한 표준 정규 노이즈 $\\varepsilon \\sim N(0, 1)$을 더한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.normal(size=10)"
      ],
      "metadata": {
        "id": "xMDyPnEZHo0o",
        "outputId": "29556245-410a-4d69-ebaa-b899c7a5c474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.74758282,  0.05169424, -2.38396155,  2.4021133 , -0.14422468,\n",
              "       -2.42640769, -0.72977893, -1.82135685,  0.78669595, -0.67984597])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-iZXJ81RGlQi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class BanditEnv(gym.Env):\n",
        "    def __init__(self, num_bandits):\n",
        "        self.num_bandits = num_bandits # 슬롯머신의 개수\n",
        "        self.action_space = list(range(num_bandits)) # 행동은 가능한 모든 슬롯머신\n",
        "        self.observation_space = [0] #상태는 의미 없음, 매 번 독립시행\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.mean = np.random.normal(size=self.num_bandits) * 10 # 10을 곱해서 평균의 크기를 키워서 보기좋게\n",
        "        return 0 # 상태 전환 없음, 의미 없음\n",
        "\n",
        "    def step(self, action):\n",
        "        state = 0\n",
        "        mean = self.mean[action]\n",
        "        reward = mean + np.random.normal()\n",
        "        done = False\n",
        "        return state, reward, done, {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPolicy:\n",
        "  def __init__(self, num_bandits):\n",
        "    self.num_bandits = num_bandits # 슬롯머신 개수 저장\n",
        "    # 이 아래에 본인이 더 저장하고 싶은 정보 정의 가능!\n",
        "    self.q = [0 for machine in range(num_bandits)]\n",
        "\n",
        "  def __call__(self, state, reward):\n",
        "    action = 0\n",
        "    ### 이 부분 수정\n",
        "    #self.q = ???\n",
        "    return action"
      ],
      "metadata": {
        "id": "qjjvGR5DOaoS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = BanditEnv(10)\n",
        "state = env.reset()\n",
        "agent = MyPolicy(10)\n",
        "reward = 0\n",
        "\n",
        "total_reward = 0\n",
        "for t in range(100):\n",
        "  action = agent(state, reward)\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  #agent.q = ???\n",
        "  print(f\"Action: {action}\")\n",
        "  print(f\"Reward: {reward}\")\n",
        "  total_reward += reward\n",
        "\n",
        "print(f\"Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6B7aphONqvY",
        "outputId": "7c2ceb61-8bf6-4fbf-b844-cf83b9470639"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: 0\n",
            "Reward: 13.871746802254949\n",
            "Action: 0\n",
            "Reward: 12.862065815090501\n",
            "Action: 0\n",
            "Reward: 12.671278424364115\n",
            "Action: 0\n",
            "Reward: 11.253084277988567\n",
            "Action: 0\n",
            "Reward: 8.647730679560354\n",
            "Action: 0\n",
            "Reward: 12.437589269577083\n",
            "Action: 0\n",
            "Reward: 12.495669923452283\n",
            "Action: 0\n",
            "Reward: 11.518751791072095\n",
            "Action: 0\n",
            "Reward: 11.172294306689425\n",
            "Action: 0\n",
            "Reward: 12.394698444554408\n",
            "Action: 0\n",
            "Reward: 9.889069345700559\n",
            "Action: 0\n",
            "Reward: 12.13078837460951\n",
            "Action: 0\n",
            "Reward: 12.596481571875064\n",
            "Action: 0\n",
            "Reward: 12.28034945764123\n",
            "Action: 0\n",
            "Reward: 13.113723345026692\n",
            "Action: 0\n",
            "Reward: 12.289790926293257\n",
            "Action: 0\n",
            "Reward: 12.77065980970352\n",
            "Action: 0\n",
            "Reward: 13.064498678600676\n",
            "Action: 0\n",
            "Reward: 11.854212316253411\n",
            "Action: 0\n",
            "Reward: 12.085141535658678\n",
            "Action: 0\n",
            "Reward: 11.293338213812877\n",
            "Action: 0\n",
            "Reward: 13.917440641159002\n",
            "Action: 0\n",
            "Reward: 11.358549049287795\n",
            "Action: 0\n",
            "Reward: 12.81649772298549\n",
            "Action: 0\n",
            "Reward: 12.240957669937503\n",
            "Action: 0\n",
            "Reward: 10.664036792837958\n",
            "Action: 0\n",
            "Reward: 11.858151227932286\n",
            "Action: 0\n",
            "Reward: 13.007406144035652\n",
            "Action: 0\n",
            "Reward: 12.293787153222217\n",
            "Action: 0\n",
            "Reward: 11.941441028432509\n",
            "Action: 0\n",
            "Reward: 13.399358432335411\n",
            "Action: 0\n",
            "Reward: 12.237293810019578\n",
            "Action: 0\n",
            "Reward: 12.500658459007008\n",
            "Action: 0\n",
            "Reward: 11.398743688597476\n",
            "Action: 0\n",
            "Reward: 10.743020690068917\n",
            "Action: 0\n",
            "Reward: 12.517555673452744\n",
            "Action: 0\n",
            "Reward: 11.33947075398643\n",
            "Action: 0\n",
            "Reward: 12.386421357915056\n",
            "Action: 0\n",
            "Reward: 11.782109281946191\n",
            "Action: 0\n",
            "Reward: 13.761828561637316\n",
            "Action: 0\n",
            "Reward: 11.82386701444011\n",
            "Action: 0\n",
            "Reward: 11.790655025526132\n",
            "Action: 0\n",
            "Reward: 11.844624150974068\n",
            "Action: 0\n",
            "Reward: 10.86495386730285\n",
            "Action: 0\n",
            "Reward: 11.581439312971188\n",
            "Action: 0\n",
            "Reward: 11.975296922368912\n",
            "Action: 0\n",
            "Reward: 13.01165434441987\n",
            "Action: 0\n",
            "Reward: 12.419568529104094\n",
            "Action: 0\n",
            "Reward: 13.334578319461773\n",
            "Action: 0\n",
            "Reward: 12.091630996477399\n",
            "Action: 0\n",
            "Reward: 13.835257473552375\n",
            "Action: 0\n",
            "Reward: 10.598681101072348\n",
            "Action: 0\n",
            "Reward: 11.476917487551235\n",
            "Action: 0\n",
            "Reward: 11.861956806110983\n",
            "Action: 0\n",
            "Reward: 10.222341690285095\n",
            "Action: 0\n",
            "Reward: 12.871238375524113\n",
            "Action: 0\n",
            "Reward: 12.46887917548566\n",
            "Action: 0\n",
            "Reward: 12.69376771941641\n",
            "Action: 0\n",
            "Reward: 11.382793862304956\n",
            "Action: 0\n",
            "Reward: 12.192724740098688\n",
            "Action: 0\n",
            "Reward: 11.560262525755322\n",
            "Action: 0\n",
            "Reward: 12.12307393032167\n",
            "Action: 0\n",
            "Reward: 12.51610723508582\n",
            "Action: 0\n",
            "Reward: 12.156194597353833\n",
            "Action: 0\n",
            "Reward: 11.80270400051292\n",
            "Action: 0\n",
            "Reward: 12.242291903047013\n",
            "Action: 0\n",
            "Reward: 13.240240754370802\n",
            "Action: 0\n",
            "Reward: 13.075879289924224\n",
            "Action: 0\n",
            "Reward: 11.658909733824125\n",
            "Action: 0\n",
            "Reward: 13.071007460974604\n",
            "Action: 0\n",
            "Reward: 13.527746351981396\n",
            "Action: 0\n",
            "Reward: 13.041396289960348\n",
            "Action: 0\n",
            "Reward: 12.252945528157369\n",
            "Action: 0\n",
            "Reward: 13.0022824237136\n",
            "Action: 0\n",
            "Reward: 11.657244955646462\n",
            "Action: 0\n",
            "Reward: 12.075573988253657\n",
            "Action: 0\n",
            "Reward: 13.228164975148541\n",
            "Action: 0\n",
            "Reward: 11.766416201039103\n",
            "Action: 0\n",
            "Reward: 11.564731808274662\n",
            "Action: 0\n",
            "Reward: 11.389293648206284\n",
            "Action: 0\n",
            "Reward: 11.823617659424016\n",
            "Action: 0\n",
            "Reward: 12.765324458530781\n",
            "Action: 0\n",
            "Reward: 13.824080180732508\n",
            "Action: 0\n",
            "Reward: 13.009331200274056\n",
            "Action: 0\n",
            "Reward: 10.857600817603029\n",
            "Action: 0\n",
            "Reward: 14.384117419900289\n",
            "Action: 0\n",
            "Reward: 10.257472581568273\n",
            "Action: 0\n",
            "Reward: 12.993322502763093\n",
            "Action: 0\n",
            "Reward: 12.221258255255416\n",
            "Action: 0\n",
            "Reward: 10.270248505374077\n",
            "Action: 0\n",
            "Reward: 11.864618267418901\n",
            "Action: 0\n",
            "Reward: 10.909755502602213\n",
            "Action: 0\n",
            "Reward: 12.187039398084073\n",
            "Action: 0\n",
            "Reward: 13.4352682818732\n",
            "Action: 0\n",
            "Reward: 13.5468931534205\n",
            "Action: 0\n",
            "Reward: 12.547346916381178\n",
            "Action: 0\n",
            "Reward: 12.976920400742376\n",
            "Action: 0\n",
            "Reward: 12.492222844102994\n",
            "Action: 0\n",
            "Reward: 12.224246607699735\n",
            "Action: 0\n",
            "Reward: 12.915747927259487\n",
            "Total Reward: 1219.6594188455858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_tLBr7cGlQj"
      },
      "source": [
        "### 슬롯머신 풀어보기\n",
        "\n",
        "직접 위 환경과 상호작용하면서, 각 슬롯머신의 보상을 추정해보세요. 그리고 Python을 이용해 위 문제에서 $\\epsilon=0.1$과 $\\epsilon=0.01$에 대해 각각 $\\epsilon$-greedy 방법을 적용해 보세요. (초기값 $Q_{1}(i)$는 모두 0으로 설정.) 또한 $Q_{1}(i) = 5$인 완전 탐욕적인 알고리즘 ($\\epsilon=0$) 또한 Python으로 구현하여 그 결과를 비교해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WKkj0BjGlQj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpsilonGreedyPolicy:\n",
        "    def __init__(self, num_bandits, epsilon):\n",
        "        self.num_bandits = num_bandits\n",
        "        self.epsilon = epsilon\n",
        "        ????\n",
        "\n",
        "    def select_action(self):\n",
        "        action = ????\n",
        "        return action\n",
        "\n",
        "\n",
        "env = BanditEnv(10)\n",
        "policy = EpsilonGreedyPolicy(10, 0.1)\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "for t in range(100):\n",
        "    action = policy.select_action()\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(\"Action:\", action, \"Reward:\", reward)\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}